#+SETUPFILE: ~/repo/org-html-themes/setup/theme-readtheorg.setup

* AWS

** Resources
[[https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/][auto scaling cheat sheet]]
[[https://serverless.com/][Create apps using serverless architecture]]

- Use cloudguru for exams
- Use linux academy for hands one
- akamai

  [[https://aws.amazon.com/route53/faqs/][faqs]]

 
** Installing CLI
[[https://docs.aws.amazon.com/cli/latest/userguide/install-macos.html#install-bundle-macos][AWS CLI install instructions]]

You can install python 3, but you have to specify the path when installing
#+BEGIN_SRC sh
brew install python
PY_PATH=$(which python3)
  
curl "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip"
unzip awscli-bundle.zip
sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws

sudo $PY_PATH awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
#+END_SRC


Or if you already are using pip:

#+BEGIN_SRC sh
pip3 install awscli --upgrade --user
#+END_SRC
If you use pip you will need to add the aws path to your $PATH
That should be something like:

#+BEGIN_SRC sh
$HOME/Library/Python/<python-version>/bin
#+END_SRC

Then configure the cli using your creds.
#+BEGIN_SRC sh
aws configure
#+END_SRC


** SQS vs SNS



|---------------------+-----------------------------------------+--------------------------------------------------|
|                     | SQS (Simple Queuing Service)            | SNS (Simple Notification Service)                |
|---------------------+-----------------------------------------+--------------------------------------------------|
| Type                | Queue                                   | Topic (Pub/Sub system)                           |
|---------------------+-----------------------------------------+--------------------------------------------------|
| Message consumption | Pull                                    | Push                                             |
|---------------------+-----------------------------------------+--------------------------------------------------|
| Use Case            | Decoupling applictions                  | Fanout                                           |
|                     | Parallel asych processsing              | Processing same message different ways           |
|---------------------+-----------------------------------------+--------------------------------------------------|
| Persistence         | Configurable                            | None                                             |
|                     | Persisted for some time                 |                                                  |
|---------------------+-----------------------------------------+--------------------------------------------------|
| Consumer Type       | Consumers are supposed to be identical. | Consumers can process messages in different ways |
|---------------------+-----------------------------------------+--------------------------------------------------|


** SQS

Amazon Simple Queue Service (SQS) is a highly scalable distributed message
queuing service provided by Amazon.

There are two types of queues

- Standard :: Nearly unlimited throughput with best-effort ordering. At-least-once delivery (You might get same message twice.)

- FIFO :: Limited number of transactions per second (TPS). See Amazon SQS FIFO developer
guide for more information on limits. Order in which messages are sent/received
is strictly preserved Exactly-once delivery

**  SQS vs SNS vs Kinesis

SQS:
- Consumer “pull data”
- Data is deleted after being consumed
- Can have as many workers (consumers) as we want
- No need to provision throughput
- No ordering guarantee (except FIFO queues)
- Individual message delay capability


SNS:
- Push data to many subscribers
- Up to 10,000,000 subscribers
- Data is not persisted (lost if not delivered)
- Pub/Sub
- Up to 100,000 topics
- No need to provision throughput
- Integrates with SQS for fan- out architecture pattern

Kinesis:
- Consumers “pull data”
- As many consumers as we want
- Possibilitytoreplaydata
- Meant for real-time big data, analytics and ETL
- Ordering at the shard level
- Data expires after X days
- Must provision throughput




** VPC - Gateway vs Interface

[[https://acloud.guru/forums/aws-csa-2019/discussion/-LbnjIbr3jdqQdRSRa7s/VPC%2520Endpoint%2520Interface%2520vs%2520Gateway][aws question]]

one technically has a private IP address and the other one is just a target

** VPC Summary
VPC Section Summary (1/2)
- CIDR: IP Range
- VPC: Virtual Private Cloud => we define a list of IPv4 & IPv6 CIDR
- Subnets: Tied to an AZ, we define a CIDR
- Internet Gateway: at the VPC level, provide IPv4 & IPv6 Internet Access
- Route Tables: must be edited to add routes from subnets to the IGW,VPC Peering Connections,VPC Endpoints, etc...
- NAT Instances: gives internet access to instances in private subnets. Old, must be setup in a public subnet, disable Source / Destination check flag
- NAT Gateway: managed by AWS, provides scalable internet access to private instances, IPv4 only
- Private DNS + Route 53: enable DNS Resolution + DNS hostnames (VPC)
- NACL: Stateless, subnet rules for inbound and outbound, don’t forget ephemeral ports
- Security Groups: Stateful, operate at the EC2 instance level


- VPC Peering: Connect two VPC with non overlapping CIDR, non transitive
- VPC Endpoints: Provide private access to AWS Services (S3, DynamoDB, CloudFormation,
- VPC Flow Logs: Can be setup at the VPC / Subnet / ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, analyze using Athena or CloudWatch Log Insights
- Bastion Host: Public instance to SSH into, that has SSH connectivity to instances in private subnets
- Site to SiteVPN: setup a Customer Gateway on DC,aVirtual Private Gateway onVPC,and site-to-site VPN over public internet
- Direct Connect:setup aVirtual Private Gateway onVPC,and establish a direct private connection to an AWS Direct Connect Location
- Direct Connect Gateway: setup a Direct Connect to many VPC in different regions
- Internet Gateway Egress: like a NAT Gateway, but for IPv6
* AWS Architect Associate


** ASG 
 ASG Default Termination Policy (simplified version):
. Find the AZ which has the most number of instances
. If there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration

 ASG tries the balance the number of instances across AZ by default
vailability zone A
 v1 v2 v1 v2
 Auto Scaling group
vailability zone B
 v1 v1 v1
    © Stephane Maarek
 ASG for Solutions Architects Scaling Cooldowns
 - The cooldown period helps to ensure that your Auto Scaling group doesn't
   launch or terminate additional instances before the previous scaling activity
   takes effect.

 In addition to default cooldown for Auto Scaling group, we can create cooldowns that apply to a specific simple scaling policy

 A scaling-specific cooldown period overrides the default cool down period.

 One common use for scaling-specific cool downs is with a scale-in policy
 a policy that terminates instances based on a specific criteria or metric.
 Because this policy terminates instances, Amazon EC2 Auto Scaling needs less
 time to determine whether to terminate additional instances.

 If the default cooldown period of 300 seconds is too long, you can reduce
 costs by applying a scaling-specific cooldown period of 180 seconds to the
 scale-in policy.

 If your application is scaling up and down multiple times each hour,modifythe
 Auto Scaling Groups cool-down timers and the CloudWatch Alarm Period that
 triggers the scale in
ttps://docs.aws.amazon.com/autoscaling/ec2/userguide/Cooldown.html

** Aurora

 - Can use IAM authentication for Aurora MySQL and PostgresSQL
   - Aurora Global databases span multiple regions and enable DR • One Primary Region
   - One DR Region
   - The DR region can be used for lower latency reads
   - < 1 second replica lag on average
 - If not using Global Databases, you can create cross-region Read Replicas
 - But the FAQ recommends you use Global Databases instead

** Disaster Recovery
- Any event that has a negative impact on a company’s business continuity or finances is a disaster
- Disaster recovery (DR) is about preparing for and recovering from a disaster
- What kind of disaster recovery?
- On-premise => On-premise: traditional DR, and very expensive • On-premise => AWS Cloud: hybrid recovery
- AWS Cloud Region A => AWS Cloud Region B
- Need to define two terms:
- RPO: Recovery Point Objective
- RTO: Recover y Time Objective
- 
*** Disaster Recovery Strategies
**** Backup and Restore (High RPO)

**** Disaster Recovery – Pilot Light
- A small version of the app is always running in the cloud
- Useful for the critical core (pilot light)
- Very similar to Backup and Restore
- Faster than Backup and Restore as critical systems are already up

**** Warm Standby
- Full system is up and running, but at minimum size
- Upon disaster, we can scale to production load
- 
**** Multi Site / Hot Site Approach
• Very low RTO (minutes or seconds) – very expensive
• Full Production Scale is running AWS and On Premise
*** Disaster RecoveryTips
**** Backup
- EBS Snapshots, RDS automated backups / Snapshots, etc...
- Regular pushes to S3 / S3 IA / Glacier, Lifecycle Policy, Cross Region Replication
- From On-Premise: Snowball or Storage Gateway
**** High Availability
- Use Route53 to migrate DNS over from Region to Region
- RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3
- Site to Site VPN as a recover y from Direct Connect
****  Replication
- RDS Replication (Cross Region), AWS Aurora + Global Databases
- Database replication from on-premise to RDS
- Storage Gateway
**** Automation
- CloudFormation / Elastic Beanstalk to re-create a whole new environment
- Recover / Reboot EC2 instances with CloudWatch if alarms fail
- AWS Lambda functions for customized automations
- Chaos
- Netflix has a “simian-army” randomly terminating EC2
  
** DynamoDB for Solutions Architect

- Operations: no operations needed, auto scaling capability, serverless
- Security
  - IAM policies
  - KMS encryption
  - SSL in flight
   
- Reliability
  - Multi AZ
  - Backups

- Performance
  - single digit millisecond writes
  - DAX for caching reads
  - performance doesn’t degrade if your application scales
   
- Cost: Pay per provisioned capacity and storage usage (no need to guess in
  advance any capacity – can use auto scaling)


** Cross Account AMI
- Sharing an AMI to other AWS accounts is possible.

- The owner of the AMI retains ownership of that AMI
  - you are the owner of the target AMI in your account.

- To copy an AMI that was shared with, the owner of the
  source AMI must grant you read permissions for the *storage that backs the AMI*,
  either the associated EBS snapshot (for an Amazon EBS-backed AMI) or an
  associated S3 bucket (for an instance store-backed AMI).

- Limits:

- Encrypted AMIS shared to your account cannot be copied.

- You can, instead, if the underlying snapshot and encryption key were shared with you,
  you can copy the snapshot while re- encrypting it with a key of your own.You
  own the copied snapshot, and can register it as a new AMI.

- You can't copy an AMI with an associated *billingProduct* code that was shared
  with you from another account.This includes Windows AMIs and AMIs from the AWS
  Marketplace.To copy a shared AMI with a *billingProduct* code, launch an EC2
  instance in your account using the shared AMI and then create an AMI from the
  instance.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html

** EBS & EFS

- EBS volumes can be attached to only one instance at a time
- EBS volumes are locked at the AZ level
- Migrating an EBS volume across AZ means first backing it up (snapshot), then recreating it in the other AZ
- EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic
- Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated. (you can disable that)
- Disk IO is high => Increase EBS volume size (for gp2) • EFS mounting 100s of instances
- EFS share website files
- EBS gp2, optimize on cost
- Custom AMI for faster deploy on ASG • EFS vs EBS vs Instance Store
- Solution architecture on EBS / EFS is discussed in a later section
* EC2
 EC2 instances are billed by the second, t2.micro is free tier

 On Linux / Mac we use SSH, on Windows we use Putty

 SSH is on port 22, lock down the security group to your IP

 Timeout issues => Security groups issues

 Permission issues on the SSH key => run “chmod 0400”

 Security Groups can reference other Security Groups instead of IP
 ranges (very popular exam question)

 Know the difference between Private, Public and Elastic IP

 You can customize an EC2 instance at boot time using EC2 User Data


  Know the 4 EC2 launch modes:
 - On demand
 - Reserved
 - Spot instances
 - Dedicated Hosts
 - Know the basic instance types: R,C,M,I,G,T2/T3

- You can create AMIs to pre-install software on your EC2 => faster boot
- AMI can be copied across regions and accounts
- EC2 instances can be started in placement groups:
   - Cluster
   - Spread

    
** ElastiCache for Solutions Architects

- Security:
  - Redis support Redis AUTH (username / password)
  - SSL in-flight encryption must be enabled and used
  - Memcached support SASL authentication (advanced)
  - None of the caches support IAM authentication
  - IAM policies on ElastiCache are only used for AWS API-level security

- Patterns for ElastiCache:
  - Lazy Loading: all the read data is cached, data can become stale in cache
  - Write Through: Adds or update data in the cache when written to a DB (no
    stale data)
  - Session Store: store temporary session data in a cache (using TTL features)

** Kinesis

*** Streams
- Divided into partitions
- Can retain data from one day up to seven days
- Can have multiple consumers
- Real-time processing
- Immutable data


*** Shards
- a stream has many shards
- 1 MB/s or 1000 messages at write *per shard*
- 2 MB/s at *read per shard*
- Billed per shard 
- Users can have unlimited shards
- Messages are ordered per shard


** RDS

 - Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE)

 - Amazon RDS supports Transparent Data Encryption for DB encryption:
    - Oracle or SQL Server DB instance only
 - TDE can be used on top of KMS – may affect performance

 - IAM Authentication (versus traditional username / password):
   -  Works for MySQL, PostgreSQL
   -  Lifespan of an authentication token is 15 minutes (short-lived)
   -  Tokens are generated by AWS credentials
   -  SSL must be used when connecting to the database
   -  Easy to use EC2 Instance Roles to connect to the RDS database
***  MOre on RDS
- *Managed* PostgreSQL / MySQL / Oracle / SQL Server
- Must provision an EC2 instance & EBS Volume type and size
- Support for Read Replicas and Multi AZ
- Security through IAM, Security Groups, KMS , SSL in transit
- Backup / Snapshot / Point in time restore feature
- Managed and Scheduled maintenance
- Monitoring through CloudWatch
- Use case: Store relational datasets (RDBMS / OLTP), perform SQL queries, transactional inserts / update / delete is available



*** Five pillars 
- Operations: small downtime when failover happens, when maintenance happens,
  scaling in read replicas / ec2 instance / restore EBS implies manual
  intervention, application changes

- Security: AWS responsible for OS security, we are responsible for setting up
  KMS, security groups, IAM policies, authorizing users in DB, using SSL

- Reliability: Multi AZ feature, failover in case of failures

- Performance: depends on EC2 instance type, EBS volume type, ability to
add Read Replicas. Doesn’t auto-scale

- Cost: Pay per hour based on provisioned EC2 and EBS
** S3 Overview
- key/value store for objects
- Great for big objects, not so great for small objects
- Serverless, scales infinitely, max object size is 5 TB
- *Eventual consistency* for overwrites and deletes
- Tiers
  - S3 Standard
  - S3 IA
  - S3 One Zone IA
  - Glacier for backups
- Features:Versioning, Encryption, Cross Region Replication, etc...
- Security: IAM, Bucket Policies, ACL
- Encryption: SSE-S3, SSE-KMS, SSE-C, client side encryption, SSL in transit


*Use Case: static files, key value store for big files, website hosting**

*** S3 for Solutions Architect

- Operations: no operations needed
- Security
  - IAM
  - Bucket Policies
  - ACL
  - Encryption (Server/Client)
  - SSL
   
- Reliability: 99.999999999% durability / 99.99% availability, Multi AZ, CRR
- Performance: scales to thousands of read / writes per second, transfer acceleration / multi-part for big files
- Cost: pay per storage usage, network cost, requests number


*** S3 Encryption for Objects

**** SSE-S3
- AWS S3 manages keys
- server-side encryption
- AES-256
- Must set header: “x-amz-server-side-encryption": "AES256"


**** SSE-KMS
- encryption using keys handled & managed by KMS
- KMS Advantages: *user control + audit trail**
- Object is encrypted server side
- Must set header: “x-amz-server-side-encryption": ”aws:kms"
 

**** SSE-C
- server-side encryption
- customer manages keys (not in aws)
- HTTPS must be used
- Encryption key must provided in HTTP headers, for every HTTP request made


**** Client Side Encryption
• Client library such as the Amazon S3 Encryption Client
• Clients must encrypt data themselves before sending to S3
• Clients must decrypt data themselves when retrieving from S3
• Customer fully manages the keys and encryption cycle



*** [[https://advancedweb.hu/2018/10/30/s3_signed_urls/%0A][How S3 Signed URLs work]]


- You can sign urls offline
  - there is no check that the resulting URL will work though

To sign:
- provide the region
- keys
- bucket
- object key for the signer
- Optionally, expiration time (defaults to 15 minutes)

** Storage Gateway

- Exam tip: Read the question well, it will hint at which gateway to use
  - On premise data to the cloud => Storage Gateway
  - File access / NFS => File Gateway
    (backed by S3)
- Volumes / Block Storage / iSCSI => Volume gateway (backed by S3 with EBS snapshots)
- VTL Tape solution / Backup with iSCSI = >Tape Gateway (backed by S3 and Glacier)
** VPC networking
*** Default VPC Walkthrough
- All new accounts have a default VPC
- New instances are launched into default VPC if no subnet is specified
- Default VPC have internet connectivity and all instances have public IP 
- We also get a public and a private DNS name

*** VPC in AWS – IPv4
- VPC =Virtual Private Cloud
- You can have multiple VPCs in a region (max 5 per region – soft limit)
- Max CIDR per VPC is 5. For each CIDR: • Min size is /28 = 16 IP Addresses
- Max size is /16 = 65536 IP Addresses
- Because VPC is private, only the Private IP ranges are allowed: 
  - 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)
  - 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)
  - 192.168.0.0 – 192.168.255.255 (192.168.0.0/16)
  - Your VPC CIDR should not overlap with your other networks (ex: corporate)
*** Comparison of Security Groups and Network ACLs

- Security Group	
Operates at the instance level

Supports allow rules only

Is stateful: Return traffic is automatically allowed, regardless of any rules

We evaluate all rules before deciding whether to allow traffic

Applies to an instance only if someone specifies the security group when
launching the instance, or associates the security group with the instance later
on

- Network ACL
Operates at the subnet level

Supports allow rules and deny rules

Is stateless: Return traffic must be explicitly allowed by rules

We process rules in number order when deciding whether to allow traffic

Automatically applies to all instances in the subnets it's associated with
(therefore, you don't have to rely on users to specify the security group)

* Terms
- <<Cross Orgin Resoure Sharing>> <<<CORS>>> :: allows you to limit the number
     of websites that can request your files in S3.

     (Lowers cost)


- Amazon DynamoDB Accelerator <<DAX>> :: a fully managed, highly available,
     in-memory cache for DynamoDB that delivers up to a 10x performance
     improvement – from milliseconds to microseconds – even at millions of
     requests per second.

- Bastion Host :: A bastion host is a specialized computer that is deliberately
                  exposed on a public network. From a secured network
                  perspective, it is the only node exposed to the outside world
                  and is therefore very prone to attack. It is placed outside
                  the firewall in single firewall systems or, if a system has
                  two firewalls, it is often placed between the two firewalls
                  or on the public side of a demilitarized zone (DMZ).
- <<<CMK>>> Customer Master Key :: The primary resources in AWS KMS are customer
     master keys (CMKs). You can use a CMK to encrypt and decrypt up to 4 KB
     (4096 bytes) of data. Typically, you use CMKs to generate, encrypt, and
     decrypt the data keys that you use outside of AWS KMS to encrypt your data.
     This strategy is known as [[envelope encryption]].


- <<envelope encryption>> Envelope Encryption ::  the practice of encrypting
     plaintext data with a data key, and then encrypting the data key under
     another key.

- Elastic Network Interface <<ENI>> ::   a logical networking component in a VPC
     that represents a virtual network card. [[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html][aws link]]

- <<KMS>> :: Amazon's Key Management System
   
- NAT Gateway ::  enables instances in a private sub-net to connect to the
                 internet or other AWS services, but prevent the internet from
                 initiating a connection with those instances [[https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html%20][aws link]]

- Recovery Point Objective (<<RPO>>) ::  How much data can your business afford to lose in a disaster.

- Recovery Time Objective (<<RTO>>) :: How much time can your business afford to be down in a disaster.
 
- <<Route 54>> :: Amazon Route 53 provides highly available and scalable Domain Name
     System (DNS), domain name registration, and health-checking web services.
     With Amazon Route 53, you can create and manage your public DNS records.

  Akamai is more widely used.

- S3 One Zone-Infrequent Access ::  (S3 One Zone-IA) S3 One Zone-IA is for data
     that is accessed less frequently, but requires rapid access when needed.

- S3 Standard-Infrequent Access <<S3 Standard-IA>>) :: S3 Standard-IA is for
     data that is accessed less frequently, but requires rapid access when
     Needed.

- Online transaction processing <<OLTP>> :: https://en.wikipedia.org/wiki/Online_transaction_processing
- <<record set>> ::

- VPC Endpoint :: nables you to privately connect your VPC to supported AWS
                  services and VPC endpoint services powered by PrivateLink
                  without requiring an internet gateway, NAT device, VPN
                  connection, or AWS Direct Connect connection.

- VPC Endpoint (Interface) :: 
- VPC Endpoint (Gateway) :: 

- VPC Peering :: networking connection between two VPCs that enables you to
                 route traffic between them using private IPv4 addresses or IPv6
                 addresses. [[https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html][aws link]]



*** Record Sets :DNS:ROUTE54:

Some common record sets: [[https://ns1.com/resources/dns-types-records-servers-and-queries][link]]

- Address Mapping record (A Record) :: AKA DNS host record, stores a hostname and its corresponding IPv4 address.

- Canonical Name record (CNAME Record) :: used to alias a hostname to another hostname.
  When a DNS client requests a record that contains a CNAME, which points to
  another hostname, the DNS resolution process is repeated with the new
     hostname.

- Certificate record (CERT Record) :: stores encryption certificates—PKIX, SPKI, PGP, and so on

- IP Version 6 Address record (AAAA Record) :: stores a hostname and its corresponding IPv6 address.

- Mail exchanger record (MX Record) :: specifies an SMTP email server for the domain, used to route
  outgoing emails to an email server.

- Name Server records (NS Record) :: specifies that a DNS Zone, such as “example.com” is delegated to
   a specific Authoritative Name Server, and provides the address of the name server.

- Reverse-lookup Pointer records (PTR Record) :: allows a DNS resolver to provide an IP address and r
  eceive a hostname (reverse DNS lookup).



- Service Location (SRV Record) :: a service location record, like MX but for other communication protocols

- Text Record (TXT Record) :: typically carries machine-readable data such as opportunistic encryption, sen
  der policy framework, DKIM, DMARC, etc.

- Start of Authority (SOA Record) ::this record appears at the beginning of a DNS zone file, and indicates
  the Authoritative Name Server for the current DNS zone, contact details for the domain administrator,
  domain serial number, and information on how frequently DNS information for this zone should be refreshed.
 




* Popular exam topics

** CORS

* Exam 1 Questions
** Question
A tech company has a CRM application hosted on an Auto Scaling group of
On-Demand EC2 instances. The application is extensively used during office hours
from 9 in the morning till 5 in the afternoon. Their users are complaining that
the performance of the application is slow during the start of the day but then
works normally after a couple of hours.

Which of the following can be done to ensure that the application works properly
at the beginning of the day?

*** Answer

Scaling based on a schedule allows you to scale your application in response to
predictable load changes. For example, every week the traffic to your web
application starts to increase on Wednesday, remains high on Thursday, and
starts to decrease on Friday. You can plan your scaling activities based on the
predictable traffic patterns of your web application.


https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/as-sample-web-architecture-diagram-with-asgs.png


To configure your Auto Scaling group to scale based on a schedule, you create a
scheduled action. The scheduled action tells Amazon EC2 Auto Scaling to perform
a scaling action at specified times. To create a scheduled scaling action, you
specify the start time when the scaling action should take effect, and the new
minimum, maximum, and desired sizes for the scaling action. At the specified
time, Amazon EC2 Auto Scaling updates the group with the values for minimum,
maximum, and desired size specified by the scaling action. You can create
scheduled actions for scaling one time only or for scaling on a recurring
schedule.

Option 3 is the correct answer. You need to configure a Scheduled scaling
policy. This will ensure that the instances are already scaled up and ready
before the start of the day since this is when the application is used the most.

Options 1 and 2 are incorrect because although this is a valid solution, it is
*still better* to configure a Scheduled scaling policy as you already know the
exact peak hours of your application. By the time either the CPU or Memory hits
a peak, the application already has performance issues, so you need to ensure
the scaling is done beforehand using a Scheduled scaling policy.

Option 4 is incorrect. Although the Application load balancer can also balance
the traffic, it cannot increase the instances based on demand.

** Question

As the Solutions Architect of the company, which of the following should you do
to meet the above requirement?


You are deploying an Interactive Voice Response (IVR) telephony system in your
cloud architecture that interacts with callers, gathers information, and routes
calls to the appropriate recipients in your company. The system will be composed
of an Auto Scaling group of EC2 instances, an Application Load Balancer, and an
RDS instance in a Multi-AZ Deployments configuration. To protect the
confidential data of your customers, you have to ensure that your RDS database
can only be accessed using the profile credentials specific to your EC2
instances via an authentication token.

As the Solutions Architect of the company, which of the following should you do
to meet the above requirement?





*** Explanation

You can authenticate to your DB instance using AWS Identity and Access
Management (IAM) database authentication. IAM database authentication works with
MySQL and PostgreSQL. With this authentication method, you don't need to use a
password when you connect to a DB instance. Instead, you use an authentication
token.

An authentication token is a unique string of characters that Amazon RDS
generates on request. Authentication tokens are generated using AWS Signature
Version 4. Each token has a lifetime of 15 minutes. You don't need to store user
credentials in the database, because authentication is managed externally using
IAM. You can also still use standard database authentication.



[[https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-13_07-04-06-a2157247b0fa129795001208504fcb51.png]]



IAM database authentication provides the following benefits:

Network traffic to and from the database is encrypted using Secure Sockets Layer
(SSL).

You can use IAM to centrally manage access to your database resources, instead
of managing access individually on each DB instance.

For applications running on Amazon EC2, you can use profile credentials specific
to your EC2 instance to access your database instead of a password, for greater
security

Hence, Option 1 is the correct answer based on the above reference.

Option 2 is incorrect because an SSL connection is not using an authentication
token from IAM. Although configuring SSL to your application can improve the
security of your data in flight, it is still not a suitable option to use in
this scenario.

Option 3 is incorrect because although you can create and assign an IAM Role to
your EC2 instances, you still need to configure your RDS to use IAM DB
Authentication.

Option 4 is incorrect because you have to use IAM DB Authentication for this
scenario, and not a combination of an IAM and STS. Although STS is used to send
temporary tokens for authentication, this is not a compatible use case for RDS.




** Question 3:
You founded a tech startup that provides online training and software
development courses to various students across the globe. Your team has
developed an online portal in AWS where the students can log into and access the
courses they are subscribed to.

Since you are in the early phases of the startup and the funding is still hard
to come by, which service can help you manage the budgets for all your AWS
resources?


** Question 4

Explanation

AWS Budgets gives you the ability to set custom budgets that alert you when your
costs or usage exceed (or are forecasted to exceed) your budgeted amount.

Budgets can be tracked at the monthly, quarterly, or yearly level, and you can
customize the start and end dates. You can further refine your budget to track
costs associated with multiple dimensions, such as AWS service, linked account,
tag, and others. Budget alerts can be sent via email and/or Amazon Simple
Notification Service (SNS) topic.

You can also use AWS Budgets to set a custom reservation utilization target and
receive alerts when your utilization drops below the threshold you define. RI
utilization alerts support Amazon EC2, Amazon RDS, Amazon Redshift, and Amazon
ElastiCache reservations.

Budgets can be created and tracked from the AWS Budgets dashboard or via the
Budgets API.

Option 1 is incorrect because the Cost Explorer only helps you visualize and
manage your AWS costs and usages over time. It offers a set of reports you can
view data with for up to the last 13 months, forecast how much you're likely to
spend for the next three months, and get recommendations for what Reserved
Instances to purchase. You use Cost Explorer to identify areas that need further
inquiry and see trends to understand your costs.

Option 2 is incorrect because Cost Allocation Tags only eases the organization
of your resource costs on your cost allocation report, to make it easier for you
to categorize and track your AWS costs.

Option 4 is incorrect because the payment history option only provides a
location where you can view the monthly invoices you receive from AWS. If your
account isn't past due, the Payment History page shows only previous invoices
and payment status.



Reference:

https://aws.amazon.com/aws-cost-management/aws-budgets/



Check out this AWS Billing and Cost Management Cheat Sheet:

https://tutorialsdojo.com/aws-cheat-sheet-aws-billing-and-cost-management/


** Question 5

A financial application that calculates accruals, interests, and other data is
hosted on a fleet of Spot EC2 instances that are configured with Auto Scaling.
The application is used by an external reporting application that provides the
total calculation for each user account and transaction. You used CloudWatch to
automatically monitor the EC2 instance without manually checking the server for
high CPU Utilization or crashes. What is the time period of data that Amazon
CloudWatch receives and aggregates from EC2 by default?






*** Explanation

By default, your instance is enabled for basic monitoring. You can
optionally enable detailed monitoring. After you enable detailed monitoring, the
Amazon EC2 console displays monitoring graphs with a 1-minute period for the
instance. The following table describes basic and detailed monitoring for
instances.

Basic - Data is available automatically in 5-minute periods at no charge.

Detailed - Data is available in 1-minute periods for an additional cost.

To get detailed data, you must specifically enable it for the instance. For the
instances where you've enabled detailed monitoring, you can also get aggregated
data across groups of similar instances.




Options 1 and 2 are incorrect because although you can publish Custom Metrics
down to 1-second or 2-second resolution to give you more immediate visibility
and greater granularity into the state and performance of your custom
applications, these are not the default values in CloudWatch.

Option 3 is incorrect because the 1-minute data period is only available for
detailed monitoring and it is not enabled by default.



References:

https://aws.amazon.com/cloudwatch/faqs/

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html



Check out this Amazon CloudWatch Cheat Sheet:

https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/




You work for a leading university as an AWS Infrastructure Engineer and also as
a professor to aspiring AWS architects. As a way to familiarize your students
with AWS, you gave them a project to host their applications to an EC2 instance.
One of your students created an instance to host their online enrollment system
project but is having a hard time connecting to their newly created EC2
instance. Your students have explored all of the troubleshooting guides by AWS
and narrowed it down to login issues.

Which of the following can you use to log into an EC2 instance?


key pairs
